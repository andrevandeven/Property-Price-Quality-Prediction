# -*- coding: utf-8 -*-
"""Property Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AMImLUeGXYroyQTPHVyujp90J1D_yLR2

# Machine Learning: Predicting Property Quality and Price
"""

# import packages
import json
import glob
import pandas as pd
import numpy as np
import datetime as dt
import re
import os
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import cm
from google.colab import drive
from sklearn.model_selection import train_test_split
from collections import Counter
import seaborn as sns

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !apt update
# !pip install kaggle

"""# **Part I:** Preprocessing and Modeling in `scikit-learn`

## **1.1** Data Loading and Preprocessing

### **1.1.1** Read and Load Data

We are using one CSV, `properties_data.csv` from a Kaggle [dataset](https://www.kaggle.com/datasets/dataregress/dubai-properties-dataset/data). The dataset contains 38 columns and over 1900 property entries.
"""

# Run this cell to mount your drive (you will be prompted to sign in)
from google.colab import drive
drive.mount('/content/drive')

!mkdir ~/.kaggle

!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/

!!kaggle datasets download -d dataregress/dubai-properties-dataset

!unzip /content/dubai-properties-dataset.zip

df_properties = pd.read_csv('properties_data.csv')

df_properties.head()

"""### **1.1.2** Understanding Data


"""

df_properties.info()

df_properties.describe()

"""## **1.2** EDA

### **1.2.1** Visualization

#### (a) Examining Neighborhood and Quality

For the ***top ten neighborhoods with the most properties***, we want to find the number of Low, Medium, High, Ultra listings from the `quality` column.
"""

top_ten_neighborhoods = df_properties[['id', 'neighborhood']].groupby('neighborhood').count().sort_values(by = 'id', ascending = False).head(10).reset_index();
top_ten_neighborhoods

ten_neighborhoods_df = top_ten_neighborhoods[['neighborhood']].merge(df_properties, on = 'neighborhood')
ten_neighborhoods_df

plt.figure(figsize = (8, 6))
sns.countplot(x = 'neighborhood', hue = 'quality', data = ten_neighborhoods_df, palette = 'husl')
plt.title('Quality of Properties in Top 10 Neighborhoods')
plt.ylabel('Count')
plt.xlabel('Neighborhood')
plt.xticks(rotation=90)

plt.show()

"""#### (b) 3D Scatterplot

We want to examine the relationship between three variables: `number_of_bedrooms`, `number_of_bathrooms`, and `price`. We also want to examine `quality` as well.
"""

import matplotlib.pyplot as plt

# save the corresponding series from dataframe into lists/containers
number_of_bedrooms = df_properties['no_of_bedrooms'].to_list()
number_of_bathrooms = df_properties['no_of_bathrooms'].to_list()
price = df_properties['price'].to_list()
quality = df_properties['quality'].to_list()

fig = plt.figure(figsize = (6, 6))
ax = fig.add_subplot(projection = '3d')

color_dict = {'Low' : 'red', 'Medium' : 'green', 'High' : 'blue', 'Ultra' : 'magenta'}

# iterate through and plot datapoints
for x_val, y_val, z_val, cat in zip(number_of_bedrooms, number_of_bathrooms, price, quality):
    ax.scatter(x_val, y_val, z_val, color=color_dict[cat])

ax.set_title('Price vs Number of Bedrooms vs Number of Bathrooms Colored By Quality')
ax.set_xlabel('Number of Bedrooms')
ax.set_ylabel('Number of Bathtrooms')
ax.set_zlabel('Price (in Millions)')

for quality, color in color_dict.items():
    ax.scatter([], [], [], c=color, marker='o', label=quality)
ax.legend(loc='upper left')
plt.show()

"""### **1.2.2** Correlation of Feature Variables

_**Isolating Numerics from Categorical Features**_
"""

for col in df_properties.columns:
  print(col, df_properties[col].nunique(), df_properties[col].dtype)

num_df = df_properties[['size_in_sqft', 'price_per_sqft', 'no_of_bedrooms', 'no_of_bathrooms']]
cat_df = df_properties.drop(['id', 'latitude', 'longitude', 'quality', 'price', 'size_in_sqft', 'price_per_sqft', 'no_of_bedrooms', 'no_of_bathrooms'], axis = 1)

sorted_columns = list(num_df.columns)
sorted_columns.sort()
num_df = num_df[sorted_columns]

sorted_columns = list(cat_df.columns)
sorted_columns.sort()
cat_df = cat_df[sorted_columns]

num_df.columns

cat_df.columns

"""_**Correlation Heatmap**_


"""

corr_matrix = num_df.corr()
plt.figure(figsize = (8, 8))
sns.heatmap(corr_matrix, annot=True, cmap='RdBu', center = 0, vmin = -1, vmax = 1)
plt.title('Correlation Heatmap Between Numeric Features')
plt.show()

"""## **1.3** Feature Engineering

### **1.3.1** Cast Boolean Values into Integers
"""

encoded_df_properties = df_properties.drop(['id', 'latitude', 'longitude', 'neighborhood'], axis = 1)

for col in encoded_df_properties:
  if encoded_df_properties[col].dtype == 'bool':
    encoded_df_properties[col] = encoded_df_properties[col].astype(int)

encoded_df_properties.info()

"""### **1.3.2** Encode Classes in 'Quality' Column

We will be predicting the `quality` for our classification problem. We first want to transform our target into numerical values:
*   Low: 0
*   Medium: 1
*   High: 2
*   Ultra: 3
"""

encoded_df_properties['quality'] = encoded_df_properties['quality'].apply(lambda x : 0 if x == 'Low' else (1 if x == 'Medium' else (2 if x == 'High' else 3)))

encoded_df_properties['quality'].unique()

"""## **1.4** Modeling (sklearn)

### **1.4.1** Preprocessing: Create Features and Label and Split Data into Train and Test
"""

features = encoded_df_properties.drop('quality', axis = 1)

target = encoded_df_properties['quality']

seed = 42
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = seed)

"""### **1.4.2** Classification Models

#### (a) Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(penalty = None, multi_class = 'multinomial')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

log_acc = model.score(X_test, y_test)

log_acc

"""#### (b) Random Forest Classifier


"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix

model = RandomForestClassifier(class_weight = 'balanced', n_estimators = 120, max_depth = 30, random_state = 42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

rf_acc = model.score(X_test, y_test)

rf_confusion = confusion_matrix(y_test, y_pred)

rf_acc, rf_confusion

"""#### (c) PCA to Reduce Dimensionality

_**Initial PCA**_
"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

pca = PCA()
X2 = pca.fit(X_train_scaled)

"""_**Cumulative Explained Variance Ratios**_


"""

explained_variance_ratios = pca.explained_variance_ratio_

cum_evr = np.cumsum(explained_variance_ratios)

x_ticks = np.arange(len(cum_evr)) + 1

plt.figure(figsize = (8, 3))
plt.plot(x_ticks, cum_evr)
plt.plot(x_ticks, 0.8 * (np.ones(len(cum_evr))))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Cumulative Explained Variance vs. Number of PCA Components')
plt.xticks(range(1, len(cum_evr) + 1, 2))
plt.show()

"""_**Final PCA**_


"""

pca = PCA(n_components = 16)
X_train_pca = pca.fit_transform(X_train_scaled)

X_test_pca = pca.transform(X_test_scaled)

"""#### (d) Logistic Regression with PCA


"""

log_reg_pca = LogisticRegression(penalty = None, multi_class = 'multinomial')
log_reg_pca.fit(X_train_pca, y_train)

y_pred = log_reg_pca.predict(X_test_pca)

test_accuracy = log_reg_pca.score(X_test_pca, y_test)

test_accuracy

"""### **1.4.3.0** Regression: Split Data into Train and Test

 We will be predicting `price` for regression models.


"""

reg_df_properties = encoded_df_properties[['price', 'size_in_sqft', 'no_of_bedrooms', 'no_of_bathrooms']]

features = reg_df_properties.drop('price', axis = 1)

target = reg_df_properties['price']

seed = 42
X_train, X_test, y_train, y_test = train_test_split(features, target, train_size = 0.8, random_state = seed)

"""### **1.4.3.1** Regression Models

#### (a) Linear Regression (Unregularized)
"""

from sklearn.linear_model import LinearRegression

reg = LinearRegression()
reg.fit(X_train, y_train)

y_pred = reg.predict(X_test)

lin_reg_score = reg.score(X_test, y_test)

lin_reg_score

"""#### (b) Lasso Regression


"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn import linear_model

reg_lasso = linear_model.Lasso(alpha = 0.5)
reg_lasso.fit(X_train_scaled, y_train)

y_pred = reg_lasso.predict(X_test_scaled)

lasso_score = reg_lasso.score(X_test_scaled, y_test)

lasso_score

"""### **1.4.4** K-Means Clustering

#### (a) Find the best number of clusters with the elbow plot
"""

features = encoded_df_properties.drop('quality', axis = 1)

from sklearn.cluster import KMeans

wcss = []
seed = 0
for k in range(2, 11):
  kmeans = KMeans(n_clusters = k, random_state = seed, n_init = 5)
  kmeans.fit(features)
  wcss.append(kmeans.inertia_)

sns.lineplot(x = [2, 3, 4, 5, 6, 7, 8, 9, 10], y = wcss)
plt.xlabel('Number of Clusters')
plt.ylabel('Distortion (WCSS)')
plt.title('Elbow Plot')
plt.show()

number_of_clusters = 5

"""#### (b) Re-fit with the best number of clusters"""

kmeans = KMeans(n_clusters = number_of_clusters, random_state = seed, n_init = 5)
kmeans.fit(features)

"""# **Part II:** Distributed Machine Learning with Apache Spark


"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !apt install libkrb5-dev
# !wget https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
# !tar xf spark-3.1.2-bin-hadoop3.2.tgz
# !pip install findspark
# !pip install sparkmagic
# !pip install pyspark
# ! pip install pyspark --user
# ! pip install seaborn --user
# ! pip install plotly --user
# ! pip install imageio --user
# ! pip install folium --user

import pyspark
from pyspark.sql import SQLContext
from pyspark.sql import SparkSession
from pyspark.sql.types import *
import pyspark.sql.functions as F

spark = SparkSession.builder.appName('bigdata-hw4').getOrCreate()
sqlContext = SQLContext(spark)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext sparkmagic.magics

# Graph section
import networkx as nx

# SQLite RDBMS
import sqlite3

import os
os.environ['SPARK_HOME'] = '/content/spark-3.1.2-bin-hadoop3.2'
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

"""## **2.1** Initializing Spark Data

### **2.1.0** Converting the Pandas Dataframe into a Spark Dataframe
"""

properties_sdf = spark.createDataFrame(encoded_df_properties)

from pyspark.sql.functions import col
properties_sdf = properties_sdf.withColumn("quality", col("quality").cast(DoubleType()))
properties_sdf.show()

properties_sdf.printSchema()

"""### **2.1.1** Setting Up a VectorAssembler


"""

all_columns = properties_sdf.columns
print(all_columns)

drop_columns = ['quality']

feature_columns = [x for x in all_columns if x not in drop_columns]
print(feature_columns)

from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(inputCols = feature_columns, outputCol = "features")

"""## **2.2** Preprocessing: Pipeline and Train-Test Split


"""

from pyspark.ml import Pipeline

pipeline = Pipeline(stages = [assembler])

pipeline_model = pipeline.fit(properties_sdf)
processed_properties_sdf = pipeline_model.transform(properties_sdf)

random_seed = 42

train_sdf, test_sdf = processed_properties_sdf.randomSplit([0.8, 0.2], seed = random_seed)

"""## **2.3** Modelling (SparkML)

### **2.3.1** Vanilla Logistic Regression
"""

from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(maxIter = 5, family = 'multinomial', labelCol = 'quality')
lr_model = lr.fit(train_sdf)

train_accuracy = lr_model.summary.accuracy

train_accuracy

predictions = lr_model.transform(test_sdf)

predictions

from pyspark.mllib.evaluation import MulticlassMetrics

columns = predictions.select('quality', 'prediction').rdd

metrics = MulticlassMetrics(columns)

confusion_matrix = metrics.confusionMatrix().toArray()
confusion_matrix

"""Now, calculate the test accuracy using the Confusion Matrix obtained above and store it in a variable called `test_accuracy`."""

total = confusion_matrix.sum()
correct = confusion_matrix.trace()
test_accuracy = correct/total
test_accuracy

"""### **2.3.2** Regularized Logistic Regression

Now, we will add regularization â€“ LASSO (L1), Ridge (L2) and elastic net (combination of L1 and L2), to avoid overfitting.

#### (a) LASSO (L1)
"""

l1_model = LogisticRegression(maxIter = 5, family = 'multinomial', labelCol = 'quality', elasticNetParam = 1, regParam = 0.03)

l1_model_fit = l1_model.fit(train_sdf)
l1_predictions = l1_model_fit.transform(test_sdf)

l1_train_accuracy = l1_model_fit.summary.accuracy

l1_train_accuracy

columns = l1_predictions.select('quality', 'prediction').rdd
metrics = MulticlassMetrics(columns)
confusion_matrix_l1 = metrics.confusionMatrix().toArray()

total = confusion_matrix_l1.sum()
correct = confusion_matrix_l1.trace()
l1_test_accuracy = correct/total

l1_test_accuracy, confusion_matrix_l1

"""#### (b) Ridge (L2)

"""

l2_model = LogisticRegression(maxIter = 5, family = 'multinomial', labelCol = 'quality', elasticNetParam = 0, regParam = 0.05)

l2_model_fit = l2_model.fit(train_sdf)
l2_predictions = l2_model_fit.transform(test_sdf)

l2_train_accuracy = l2_model_fit.summary.accuracy
l2_train_accuracy

columns = l2_predictions.select('quality', 'prediction').rdd
metrics = MulticlassMetrics(columns)
confusion_matrix_l2 = metrics.confusionMatrix().toArray()

total = confusion_matrix_l2.sum()
correct = confusion_matrix_l2.trace()
l2_test_accuracy = correct/total
l2_test_accuracy, confusion_matrix_l2

"""#### (c) Elastic Net


"""

en_model = LogisticRegression(maxIter = 5, family = 'multinomial', labelCol = 'quality', elasticNetParam = 0.5, regParam = 0.05)

en_model_fit = en_model.fit(train_sdf)
en_predictions = en_model_fit.transform(test_sdf)

en_train_accuracy = en_model_fit.summary.accuracy

en_train_accuracy

columns = en_predictions.select('quality', 'prediction').rdd
metrics = MulticlassMetrics(columns)
confusion_matrix_en = metrics.confusionMatrix().toArray()

correct = confusion_matrix_en.trace()
total = confusion_matrix_en.sum()
en_test_accuracy = correct/total
en_test_accuracy, confusion_matrix_en

"""### **2.3.3** Random Forest Classification


"""

from pyspark.ml.classification import RandomForestClassifier

random_seed = 42

rf = RandomForestClassifier(maxDepth = 10, seed = random_seed, labelCol = 'quality')
rf_model = rf.fit(train_sdf)

train_pred = rf_model.transform(train_sdf)
test_pred = rf_model.transform(test_sdf)

columns = train_pred.select('quality', 'prediction').rdd
metrics = MulticlassMetrics(columns)
rf_train_cm = metrics.confusionMatrix().toArray()
correct = rf_train_cm.trace()
total = rf_train_cm.sum()
rf_train_accuracy = correct/total

columns = test_pred.select('quality', 'prediction').rdd
metrics = MulticlassMetrics(columns)
rf_test_cm = metrics.confusionMatrix().toArray()
correct = rf_test_cm.trace()
total = rf_test_cm.sum()
rf_test_accuracy = correct/total

rf_train_accuracy, rf_test_accuracy

"""### **2.3.4** Dimensionality Reduction Using PCA


"""

from pyspark.ml.feature import PCA, StandardScaler

scaler = StandardScaler(inputCol = "features", outputCol = "scaledFeatures")
model = scaler.fit(train_sdf)
train_scaled = model.transform(train_sdf)
test_scaled = model.transform(test_sdf)

pca = PCA(k = 16, inputCol = "scaledFeatures")
pca_model = pca.fit(train_scaled)
train_pca = pca_model.transform(train_scaled)
test_pca = pca_model.transform(test_scaled)

lr_model = LogisticRegression(maxIter = 5, regParam = 0.05, family = 'multinomial', labelCol = 'quality')

lr_model = lr_model.fit(train_pca)
test_pred = lr_model.transform(test_pca)

train_accuracy_pca = lr_model.summary.accuracy
train_accuracy_pca

columns = test_pred.select('quality', 'prediction').rdd
metrics = MulticlassMetrics(columns)
confusion_matrix_pca = metrics.confusionMatrix().toArray()

correct = confusion_matrix_pca.trace()
total = confusion_matrix_pca.sum()
test_accuracy_pca = correct/total
test_accuracy_pca, confusion_matrix_pca